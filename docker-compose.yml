version: "3.7"

services:
  llm:
    image: ghcr.io/tgusciora/snacksize_server:latest
    container_name: llama-server
    restart: always
    # 1. Use Host Network
    network_mode: "host"
    volumes:
      - ./models:/models
    environment:
      - LLAMA_API_KEY=${API_KEY}
    command: >
      -m /models/LFM2-1.2B-RAG-Q4_K_M.gguf
      --host 127.0.0.1
      --port 8080
      --api-key ${API_KEY}
      -c 20048

  caddy:
    image: caddy:2-alpine
    restart: always
    # 3. Use Host Network
    network_mode: "host"
    # Note: No 'ports' section allowed here. Caddy listens on whatever ports
    # are defined in the Caddyfile (80, 443, 20137).
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config

volumes:
  caddy_data:
  caddy_config:
